\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{fullpage}
\usepackage[shortlabels]{enumitem}


\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\newcommand{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}
\newcommand{\RE}{\mathrm{Re}}
\newcommand{\IM}{\mathrm{Im}}

\setlength\parindent{0pt}
\begin{document}

From "Principles of Applied Mathematics" by James P. Keener.

\section*{Chapter 1 Finite Dimensional Vector Spaces}

\subsection*{Section 1.1 Linear Vector Spaces}

\subsubsection*{Problem 1}

Show that in any inner product space $||x+y||^2 + ||x-y||^2 = 2||x||^2 + 2||y||^2$.\\

Answer:
\begin{align*}
||x+y||^2 + ||x-y||^2 &= \langle x+y, x+y \rangle + \langle x-y , x-y\rangle \\
&= \langle x, x+y \rangle + \langle y, x+y \rangle + \langle x , x-y \rangle - \langle y, x-y \rangle \\
&= \overline{\langle x + y , x \rangle} + \overline{\langle x+y, y \rangle} + \overline{\langle x - y , x \rangle} - \overline{\langle x-y, y \rangle} \\
&= \overline{\langle x, x \rangle + \langle y, x \rangle} + \overline{\langle x , y \rangle + \langle y , y \rangle}+ \overline{\langle x , x \rangle - \langle y , x \rangle} - \overline{\langle x , y \rangle - \langle y , y \rangle} \\
&= \overline{\langle x, x \rangle} + \overline{\langle y, x \rangle} + \overline{\langle x , y \rangle} + \overline{\langle y , y \rangle}+ \overline{\langle x , x \rangle} - \overline{\langle y , x \rangle} - \overline{\langle x , y \rangle} + \overline{\langle y , y \rangle} \\
&= 2\overline{\langle x, x \rangle} +  2\overline{\langle y , y \rangle} \\
&= 2\langle x , x\rangle + 2 \langle y, y \rangle \\
&= 2||x||^2 + 2||y||^2.
\end{align*}

\subsubsection*{Problem 5}

Verify that the choice $\alpha = \frac{\langle x , y \rangle}{||y||^2}$ makes $||x - \alpha y||^2$ as small as possible. Show that $|\langle x , y \rangle |^2 = ||x||^2 ||y||^2$ if and only if $x$ and $y$ are linearly dependent. 

\begin{align*}
0 &= \frac{\partial}{\partial \alpha} ||x - \alpha y||^2 \\
&= \frac{\partial}{\partial \alpha} \left[ ||x||^2 - 2\RE\left( \langle x , \alpha y \rangle \right) + |\alpha|^2 ||y||^2 \right] \\
&= 0 - 2\frac{\partial}{\partial \alpha} \RE \left(\overline{\alpha} \langle x , y \rangle \right) + 2\alpha ||y||^2 \\
&= - 2 \RE \left( \frac{\partial}{\partial \alpha} \overline{\alpha}\langle x , y \rangle \right) + 2\alpha ||y||^2 \\
&= - 2 \RE \left( \overline{\frac{\partial}{\partial \alpha} \alpha \overline{\langle x , y \rangle} }\right) + 2\alpha ||y||^2 \\
&= - 2 \RE \left(\overline{\overline{\langle x , y \rangle}} \right) + 2\alpha ||y||^2 \\
&= - 2 \RE \left(\langle x , y \rangle \right) + 2\alpha ||y||^2 \\
\implies \alpha &= \frac{\RE \langle x , y \rangle}{||y||^2}.
\end{align*}

Since $||x - y||^2$ is quadratic in $\alpha$ with $||y||^2 > 0$, this value of $\alpha$ minimizes $||x - \alpha y ||^2$. If $x$ and $y$ are real valued vectors or if at least $\langle x , y \rangle$ is a real number, $\alpha = \langle x , y \rangle / ||y||^2$ makes $||x - \alpha y||^2$ as small as possible. 

\newpage
\subsection*{Section 1.2 Spectral Theory for Matrices}

\subsubsection*{Problem 2}

Prove that two symmetric matrices are equivalent if and only if they have the same eigenvalues. \\

Answer: Suppose that $A$ and $B$ are symmetric matrices and that $A = M^{-1}BM$. Suppose $\lambda$ is an eigenvalue of $A$ with corresponding eigenvector $v$.

\begin{align*}
Av &= \lambda v \\
M^{-1}BMv &= \lambda v \\
B(Mv) &= \lambda (Mv)
\end{align*}

This shows that $\lambda$ is also an eigenvalue of $B$ (and that $Mv$ is an eigenvector corresponding to $\lambda$). A similar calculation shows that any eigenvalue of $B$ is also an eigenvalue of $A$. Conclude that the similar matrices $A$ and $B$ have the same eigenvalues. \\

Now suppose that $A$ and $B$ have the same eigenvalues. Since $A$ is symmetric, $A = C^{-1}\Lambda C$ where $\Lambda$ is the diagonal matrix containing the eigenvalues of $A$ (we can assume that the eigenvalues in $\Lambda$ can be put into descending order using permutation matrices if necessary) and $C$ contains eigenvectors corresponding to the eigenvalues of $A$. Since $B$ is symmetric and has the same eigenvalues as $A$, $B = D^{-1}\Lambda D$ where $D$ contains the eigenvectors corresponding to the eigenvalues of $B$. 

$$
CAC^{-1} = \Lambda = DBD^{-1} \implies A = C^{-1}DBD^{-1}C = (D^{-1}C)^{-1}B(D^{-1}C) = M^{-1}BM, \quad M = D^{-1}C.
$$

\subsubsection*{Problem 7}

Find the spectral representation of the matrix 

$$ 
A = \begin{pmatrix}
7 & 2 \\ -2 & 2
\end{pmatrix} \;.
$$

Illustrate how $Ax = b$ can be solved geometrically using the appropriately chosen coordinate system on a piece of graph paper. \\

Answer: Find the eigenvalues of $A$: $0 = (7-\lambda)(2-\lambda) + 4 = (\lambda - 6)(\lambda - 3) \implies \lambda = 6,3$. Take $x = (2,-1)^T / \sqrt{3}$ as an eigenvector corresponding to $\lambda = 6$ and $y = (1, -2)^T/\sqrt{3}$ as an eigenvector corresponding to $\lambda = 3$. Let $M = (x , y)$. A spectral representation of $A$ is given by

$$
A = M^{-1} \Lambda M = \frac{1}{3}
\begin{pmatrix}
2 & 1 \\ -1 & -2
\end{pmatrix}^{-1}
\begin{pmatrix}
6 & 0 \\ 0 & 3
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\ -1 & -2
\end{pmatrix}
= \frac{1}{3}\begin{pmatrix}
2 & 1 \\ -1 & -2
\end{pmatrix}
\begin{pmatrix}
6 & 0 \\ 0 & 3
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\ -1 & -2
\end{pmatrix}.
$$

Solving $Az = b$ is the same as solving $\Lambda (Mz) = Mb$ for $Mz = z'$ and then calculating $z = M^{-1}z'$. Since $\Lambda$ is diagonal, the $i$th element of $Mz$ is found by dividing the $i$th element of $Mb$ by $\lambda_i$. Choose the coordinate system that has the eigenvectors $x$ and $y$ as axes so that solving any system amounts to solving $\Lambda z' = b'$. That is, find the coordinates of $b'$ with respect to $x$ and $y$. 

\subsubsection*{Problem 9}

The sets of vectors $\{\phi_i \}_{i=1}^n, \{\psi_i \}_{i=1}^n$ are biorthogonal if $\langle \phi_i , \psi_j \rangle = \delta_{ij}$. Suppose $\{\phi_i \}_{i=1}^n$ and $\{\psi_i \}_{i=1}^n$ are biorthogonal.

\begin{enumerate}[a.]
\item Show that $\{\phi_i \}_{i=1}^n$ and $\{\psi_i \}_{i=1}^n$ each form a linearly independent set.

\item
Show that any vector in $\mathbb{R}^n$ can be written as a linear combination of $\{\phi_i\}$ as

$$
x = \sum_{i=1}^n \alpha_i \phi_i, \quad \alpha_i = \langle x, \psi_i\rangle.
$$

\item
Re-express the result from part b in matrix form; that is, show that

$$
x = \sum_{i=1}^n P_i x
$$

where $P_i$ are projection matrices with the properties that $P_i^2 = P_i$ and $P_iP_j = 0$ for $i \neq j$. Express the matrix $P_i$ in terms of the vectors $\phi_i$ and $\psi_i$. 
\end{enumerate}

Answer:

\begin{enumerate}[a.]

\item 

Suppose $\sum_{i= 1}^n \alpha_i \phi_i = 0$. For $j = 1,\dots , n$, taking the inner product with $\psi_j$ on each side of this equation gives:

$$
\alpha_j = \alpha_j \langle\phi_j , \psi_j \rangle
 = \bigg\langle \sum_{i = 1}^n \alpha_i \phi_i, \psi_j \bigg\rangle = \langle 0, \psi_j \rangle = 0.
$$

The only way to write the zero vector as a linear combination of the $\phi_i$ is when all coefficients $\alpha_i = 0$. Therefore $\{\phi_i \}_{i=1}^n$ is a linearly independent set. A similar calculation shows that if $\sum_{i=1}^n \beta_i \psi_i = 0$, then $\beta_i = 0$ for each $i$. Therefore $\{\psi_i \}_{i=1}^n$ is also a linearly independent set. 

\item This was not explicitly stated but assuming that the $\phi_i$ are vectors in $\mathbb{R}^n$, $\{\phi_i \}_{i=1}^n$ is a set of $n$ linearly independent vectors in $\mathbb{R}^n$ so the vectors in $\{\phi_i \}_{i=1}^n$ form a basis for $\mathbb{R}^n$. For any $x \in \mathbb{R}^n$, there exist $\alpha_i$, $i = 1,\dots , n$ such that $x = \sum_{i=1}^n \alpha_i \phi_i$. Use the biorthogonality of $\{\phi_i \}_{i=1}^n$ and $\{\psi_i \}_{i=1}^n$ to determine the value of each $\alpha_i$.

$$
\langle x , \psi_i \rangle = \bigg \langle \sum_{k=1}^n \alpha_k \phi_k , \psi_i \bigg \rangle = \langle \alpha_i \phi_i , \psi_i \rangle = \alpha_i, \quad \text{for each }i = 1,\dots , n. 
$$

\item 

\begin{align*}
x &= \sum_{i=1}^n \alpha_i \phi_i \\
&= \sum_{i=1}^n \langle x , \psi_i \rangle \phi_i \\
&= \begin{pmatrix}
\phi_1 & \dots & \phi_n
\end{pmatrix}
\begin{pmatrix}
\langle x , \psi_1 \rangle \\
\vdots \\
\langle x , \psi_n \rangle
\end{pmatrix} \\
&= \begin{pmatrix}
\phi_1 & \dots & \phi_n
\end{pmatrix}
\begin{pmatrix}
\psi_1^T \\
\vdots \\
\psi_n^T
\end{pmatrix} x\\
&= \sum_{i=1}^n \phi_i \psi_i^T x \\
&= \sum_{i=1}^n P_i x, \quad P_i = \phi_i \psi_i^T
\end{align*}

$P_i^2 = (\phi_i\psi_i^T)(\phi_i\psi_i^T) = \phi_i(\psi_i^T\phi_i)\psi_i^T) = \phi_i \psi_i^T = P_i$ and $P_iP_j = \phi_i\psi_i^T\phi_j\psi_j^T = \phi_i \cdot 0 \cdot \psi_j^T = 0$ for $i \neq j$. 

\end{enumerate}


\subsubsection*{Problem 10}

\begin{enumerate}[a.]
\item
Suppose the eigenvalues of $A$ are distinct. Show that the eigenvectors of $A$ and the eigenvectors of $A^*$ form a biorthogonal set. 

\item
Suppose $A\phi = \lambda_i \phi_i$ and $A^* \psi_i = \overline{\lambda_i}\psi_i$, $i = 1,\dots , n$ and that $\lambda_i \neq \lambda_j$ for $i \neq j$. Prove that $A = \sum_{i=1}^n \lambda_i P_i$ where $P_i = \phi_i \psi_i^*$ is a projection matrix. 

\item Express the matrices $C$ and $C^{-1}$ where $A = C \Lambda C^{-1}$, in terms of $\phi_i$ and $\psi_i$. 
\end{enumerate}

Answer: 

\begin{enumerate}[a.]
\item If $\lambda$ is an eigenvalue of $A$ then $\overline{\lambda}$ is an eigenvalue of $A^*$. 

$$
\lambda_j \langle \phi_i, \psi_j \rangle 
= \langle \phi_i, \overline{\lambda_j} \psi_j \rangle
= \langle \phi_i, A^* \psi_j \rangle 
= \langle A\phi_i,\psi_j \rangle 
= \langle \lambda_i \phi_i ,\psi_j \rangle
= \lambda_i \langle \phi_i , \psi_j \rangle
\implies (\lambda_j - \lambda_i) \langle \phi_i , \psi_j \rangle = 0
$$

From this conclude that $\langle \phi_i,\psi_j \rangle = 0$ if $\lambda_i \neq \lambda_j$.\\

Since $A$ has $n$ distince eigenvalues, $A^*$ has $n$ distinct eigenvalues. By Theorems 1.2 and 1.3 both are diagonalizable.

\begin{align*}
\Lambda & =M^{-1}AM \\
\Lambda^* &= M^* A^* (M^{-1})^* = N^{-1}A^* N, \quad N= (M^{-1})^*\\
\end{align*}

$M$ contains the eigenvectors $\{\phi_i\}_{i=1}^n$ of $A$ and $N$ contains the eigenvectors $\{\psi_i\}_{i=1}^n$ of $A^*$. 

$$
I = M^{-1}M = N^*M = \begin{pmatrix}
\psi_1^* \\ \vdots \\ \psi_n^*
\end{pmatrix}
\begin{pmatrix}
\phi_1 & \dots & \phi_n
\end{pmatrix}
= \begin{pmatrix}
\langle \psi_1, \phi_1\rangle & & \\
 & \ddots & \\
  & & \langle \psi_n, \phi_n \rangle
\end{pmatrix}
$$

Conclude that $\langle \psi_i , \phi_i \rangle = 1$ for $i = 1,\dots , n$. 

\item

$$
A = M\Lambda M^{-1} = M \Lambda N^* 
= \begin{pmatrix}
\phi_1  & \dots & \phi_n
\end{pmatrix}
\begin{pmatrix}
\lambda_1 & & \\
& \ddots & \\
& & \lambda_n
\end{pmatrix}
\begin{pmatrix}
\psi_1^* \\ \vdots \\ \psi_n^*
\end{pmatrix}
= \sum_{i=1}^n \lambda_i \phi_i \psi_i^*= \sum_{i = 1}^n \lambda_iP_i.
$$

The $P_i$ are projection matrices since $P_i^2 = P_i$. 

\item

Replacing $M$ with $C$ in the work above,

$$
A = C\Lambda C^{-1}, \quad C = \begin{pmatrix}
\phi_1  & \dots & \phi_n
\end{pmatrix}, \quad C^{-1} = \begin{pmatrix}
\psi_1^* \\ \vdots \\ \psi_n^*
\end{pmatrix}
$$

\end{enumerate}


\newpage
\subsection*{Section 1.3 Geometrical Significance of Eigenvalues}

\subsubsection*{Problem 3}

Show that the intermediate eigenvalue of $\lambda_2$ of $A$ is not positive when

$$
A = \begin{pmatrix}
1 & 2 & 3 \\ 2 & 2 & 4 \\ 3 & 4 & 3
\end{pmatrix}.
$$

Answer: No complete answer was found. First I calculated $Q(x) = \langle Ax , x \rangle$, which gives a quadratic form in $x_1, x_2, x_3$. Then converted to polar coordinates but with $\rho = 1$ and calculated $\frac{\partial{Q}}{\partial \phi}, \frac{\partial{Q}}{\partial \theta}$. This got too complicated to continue without more time.  

\subsection*{Section 1.4 Fredholm Alternative Theorem}

\subsubsection*{Problem 1}

Under what conditions do these matrices have solutions $Ax = b$? Are they unique?

\begin{enumerate}[a.]
\item

$$
A = \begin{pmatrix}
1 & 2 \\ 1 & 3 \\ 2 & 5
\end{pmatrix}
$$

\item
$$
A = \begin{pmatrix}
1 & 2 & 3 \\ 3 & 1 & 2 \\ 1 & 1 & 1
\end{pmatrix}
$$
\end{enumerate}

Answer: By Theorem 1.10 (Fredholm Alternative), $Ax = b$ has a solution if and only if $\langle b , v \rangle = 0$ for every vector $v$ satisfying $A^*v = 0$. By Theorem 1.9, the solution to $Ax = b$ (if it exists) is unique if and only if the only solution to $Ax = 0$ is $x = 0$. 

\begin{enumerate}[a.]
\item

Find the nullspace of $A^*$. That is, find the set of vectors $v$ such that $A^*v = 0$. 
\begin{align*}
(A^* \vert 0) &= \begin{pmatrix}
1 & 1 & 2 & 0 \\ 2 & 3 & 5 & 0
\end{pmatrix}
\sim 
\begin{pmatrix}
1 & 0 & 1 & 0 \\ 
0 & 1 & 1 & 0
\end{pmatrix}
\implies v = r\begin{pmatrix}
1 \\ 1 \\ -1
\end{pmatrix}, \quad r \in \mathbb{C}. \\
0 &= \langle b, v \rangle = r(b_1 + b_2 - b_3)
\end{align*}

The equation $Ax = b$ has a solution if and only if the components of $b$ satisfy $b_1 + b_2 - b_3 = 0$. The solution $x$ of $Ax =b$ is unique if and only if the equation $Ay = 0$ has $y = 0$ as its only solution. 

\begin{align*}
(A \vert 0) &= \begin{pmatrix}
1 & 2 & 0 \\ 1 & 3  & 0\\ 2 & 5 & 0
\end{pmatrix}
\sim 
\begin{pmatrix}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0
\end{pmatrix} \implies y = 0.
\end{align*}

There exists a unique solution $x$ of the equation $Ax = b$ if $b_1 + b_2 - b_3 = 0$. 

\item

\begin{align*}
(A^* \vert 0) &= \begin{pmatrix}
1 & 3 & 1 & 0 \\ 2 & 1 & 1 & 0 \\ 3 & 2 & 1 & 0
\end{pmatrix}
\sim
(I \vert 0) \implies v = 0.
\end{align*}

The nullspace of $A^*$ contains only the zero vector. This means that for any $b$, $0 = \langle b, v\rangle$ for every $v$ such that $A^*v = 0$. The equation $Ax = b$ has a solution for any $b$. 

\begin{align*}
(A \vert 0) &= 
\begin{pmatrix}
1 & 2 & 3  & 0\\ 3 & 1 & 2 & 0 \\ 1 & 1 & 1 & 0
\end{pmatrix} \sim (I | 0) \implies y = 0.
\end{align*}

The only solution to $Ay = 0$ is $y = 0$. Conclude that for any $b$ there exists a unique solution $x$ of the equation $Ax = b$. 

\end{enumerate}

\subsubsection*{Problem 4}

A square matrix $A \in \mathbb{R}^{n\times n}$ is positive definite if $\langle Ax , x \rangle > 0$ for all $x \neq 0$. Use the Fredholm alternative to prove that a positive definite matrix is uniquely invertible. \\

Answer: Suppose $0 = A^*v = A^Tv$. Then $0 = v^TA^Tv = \langle Av, v \rangle$. Since $A$ is positive definite, $v = 0$. For any $b \in \mathbb{R}^n$, $0 = \langle b , v \rangle = \langle b , 0 \rangle$. By the Fredholm alternative there exists a solution $x$ of the equation $Ax = b$ for any $b \in \mathbb{R}^n$. In particular there exist $x_1, \dots , x_n$ such that $Ax_i = e_i$ for $i = 1,\dots , n$ where $e_i$ is the $i$th standard basis vector for $\mathbb{R}^n$. Suppose $Ay = 0$. Then $0 = y^T Ay = \langle y , Ay \rangle = \overline \langle Ay , y \rangle = \langle Ay , y \rangle$. Since $A$ is positive definite, $y = 0$. For $i = 1, \dots , n$ there exists by the above and Theorem 1.9 a unique solution $x_i$ of the equation $Ax_i = e_i$. Therefore $A$ is uniquely invertible with $A^{-1} = \begin{pmatrix} x_1 & \dots & x_n\end{pmatrix}$. 


\newpage
\subsection*{Section 1.5 Least Squares Solutions - Pseudo Inverses}

\subsubsection*{Problem 2}

Verify that the pseudo-inverse of an $m \times n$ diagonal matrix $D$ with $d_{ij} = \sigma_i \delta_{ij}$ is the $n \times m$ diagonal matrix $D'$ with $d_{ij}' = \frac{1}{\sigma_i}\delta_{ij}$ whenever $\sigma_i \neq 0$ and $d_{ij}' = 0$ otherwise. \\

Answer: The definition of a least square pseudo inverse $A'$ of $A$ requires that $x = A'b$ satisfies:

\begin{enumerate}
\item $A^*Ax = A^*b$ 

\item $\langle x , w \rangle = 0$ for every $w$ satisfying  $Aw = 0$. 
\end{enumerate}

If $\sigma_i \neq 0$ for each $i$, 
\begin{align*}
D^*Dx &= D^* b \\
D^*DD'b &= D^* b \\
\begin{pmatrix}
|\sigma_1|^2 & & \\
& \ddots & \\
& & |\sigma_n|^2
\end{pmatrix}
D'b &= D^*b \\
\begin{pmatrix}
|\sigma_1|^2 & & \\
& \ddots & \\
& & |\sigma_n|^2
\end{pmatrix}
\begin{pmatrix}
1/\sigma_1 & & \\ & \ddots & \\ & & 1/\sigma_n
\end{pmatrix}b &= \begin{pmatrix}
\sigma_1^* & & \\ & \ddots & \\ & & \sigma_n^*
\end{pmatrix} b
\end{align*}

This shows that $|\sigma_i|^2 b_i / \sigma_i = \sigma_i^* b_i$, which is equivalent to $|\sigma_i|^2 b_i = |\sigma_i|^2 b_i$, which is of course true. There may be an additional $r = m-n$ columns in $D'$ and $D^*$ if $m > n$. Since these are $r$ columns of zeros, multiplication by $b$ just causes the last $r$ components to vanish so that for these components we only need to verify that $0 = 0$, which is of course true. Finally, if there is a $\sigma_i = 0$, we have instead of $1/\sigma_i$ a 0 at the $i$th diagonal entry of $D'$. This causes the $i$th component of the left hand side of the equation to vanish. But also $\sigma_i^* = 0$ in this case so that the same occurs on the right hand side of this equation, so that we again only confirm that $0 = 0$. Therefore $x = D'b$ satisfies (1). \\

Suppose $w$ satisfies $Dw = 0$:

$$
\begin{pmatrix}
0 \\ \vdots \\ 0
\end{pmatrix}
= 
\begin{pmatrix}
\sigma_1 & & \\ & \ddots & \\ & & \sigma_n
\end{pmatrix}
\begin{pmatrix}
w_1 \\ \vdots \\ w_n
\end{pmatrix}.
$$

This shows that $w_i = 0$ whenever $\sigma_i \neq 0$. 

$$
\langle x , w \rangle = \langle D'b , w \rangle = \langle b^*, (D')^*w \rangle = \begin{pmatrix}
b_1^* & \dots & b_n^* 
\end{pmatrix}
\begin{pmatrix}
(d_{11}')^* & & \\ & \ddots & \\ & & (d_{nn}')^*
\end{pmatrix}
\begin{pmatrix}
w_1 \\ \vdots \\ w_n
\end{pmatrix},
$$

where $(d_{ii}')^* = \left(1/\sigma_i\right)^*$ for $\sigma_i \neq 0$ and $(d_{ii}')^* = 0^* = 0$ if $\sigma_i = 0$. But this means $(D')^*w = 0$ so that $\langle x, w \rangle = b^*(D')^*w = 0$. Therefore $x = D'b$ satisfies (2) as well. Conclude that $D'$ is a least square pseudo inverse of $D$. 

\subsubsection*{Problem 9}

Find a singular value decomposition and pseudo-inverse of 

$$
A = \begin{pmatrix}
2\sqrt{5} & -2\sqrt{5} \\
3 & 3 \\
6 & 7
\end{pmatrix} \;.
$$

Answer:

$$
A^TA = \begin{pmatrix}
65 & 31 \\ 31 & 78
\end{pmatrix} \quad \text{has eigenvalues } \sigma^2 = \frac{1}{2}(143 \pm \sqrt{4013})
$$

$$
v_1 = \left(\frac{1}{62}\left(\sqrt{4013} - 13\right), 1\right)^T / \left(\frac{\sqrt{4013} - 13}{62}\right)
$$
$$
v_2 = \left(\frac{1}{62}\left(-\sqrt{4013} - 13\right), 1\right)^T / \left(\frac{\sqrt{4013} + 13}{62}\right)
$$
$$
V = \begin{pmatrix}
v_1 & v_2 
\end{pmatrix}, \quad V^* = V^T = \begin{pmatrix}
v_1^T \\ v_2^T
\end{pmatrix}
$$

Since $V^T V = VV^T = I$, $V$ is an orthogonal matrix.

$$
AA^T = \begin{pmatrix}
40 & 0 & -2\sqrt{5} \\ 0 & 18 & 39 \\ -2\sqrt{5} & 39 & 85
\end{pmatrix} \quad \text{has eigenvalues } \sigma^2 = 0,  \frac{1}{2}(143 \pm \sqrt{4013})
$$

$$
u_1 \approx (-0.0707906, 0.457886, 1)^T / ||-0.0707906, 0.457886, 1)^T||
$$
$$
u_2 \approx (25.6839, 1.78687, 1)^T / ||(25.6839, 1.78687, 1)^T||
$$
$$
u_3 \approx (0.111803, -2.16667, 1)^T / ||(0.111803, -2.16667, 1)^T||
$$
$$
U = \begin{pmatrix}
u_1 & u_2 & u_3
\end{pmatrix}, \quad U^* = U^T = \begin{pmatrix}
u_1^T \\ u_2^T \\ u_3^T
\end{pmatrix}
$$

We have $UU^T \approx I$ and $U^TU \approx I$. If we had $u_1, u_2, u_3$ exactly this would become $UU^T = U^TU = I$ so that $U$ is orthogonal as well. A full SVD of $A$ is given by

$$
A = U \Sigma V^T = \begin{pmatrix}
u_1 & u_2 & u_3 
\end{pmatrix}
\begin{pmatrix}
\sigma_1 & 0 & 0 \\
0 & \sigma_2 & 0 \\
0 & 0 & 0
\end{pmatrix}
\begin{pmatrix}
v_1^T \\ v_2^T
\end{pmatrix}, \quad
\sigma_1 = \sqrt{\frac{1}{2}\left(143+\sqrt{4013}\right)}, \quad \sigma_2 = \sqrt{\frac{1}{2}\left(143-\sqrt{4013}\right)}.
$$

Since $A$ has three rows and two columns, $A$ cannot have a left right inverse. But since the columns of $A$ are independent, $A$ has full column rank so that $Ax = b$  has at most one solution $x$ for every $b$. Then $A$ has a $2 \times 3$ left inverse $B$ such that $BA = I_n$. 

\begin{align*}
B &= (A^TA)^{-1}A^T \\
&= \begin{pmatrix}
65 & 31 \\ 31 & 78
\end{pmatrix}^{-1} \begin{pmatrix}
2\sqrt{5} & 3 & 6 \\
-2\sqrt{5} & 3 & 7
\end{pmatrix}\\
&= \frac{1}{4109} \begin{pmatrix}
218\sqrt{5} & 141 & 251 \\ -192\sqrt{5} & 102 & 269
\end{pmatrix}
\end{align*}

Performing the calculation $BA$ does indeed give $I_n$ as claimed. 

\end{document}